{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forecasting_multivariate_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzncR+pqsQEj7eVHMWntGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiaufrrj/BRtrading/blob/main/forecasting_multivariate_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oeGlpRH53xA"
      },
      "source": [
        "referÃªncia: https://github.com/vb100/multivariate-lstm/blob/master/LSTM_model_stocks.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_G0hSwa6J0p"
      },
      "source": [
        "Precisa adaptar para "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HN7ueeV4z6U"
      },
      "source": [
        "PART 1. Data Pre-processing\n",
        "Step #0. Fire the system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv5x-8vY732H"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as46NC1o4zBP"
      },
      "source": [
        "# Import modules and packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4qkitZ_46Ta"
      },
      "source": [
        "\n",
        "Step #1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajs9yaK64017"
      },
      "source": [
        "# Importing Training Set\n",
        "dataset_train = pd.read_csv('https://raw.githubusercontent.com/vb100/multivariate-lstm/master/GOOG.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI8BsNbl6iE4",
        "outputId": "93cb5759-37e8-4a9b-c738-b76f6e3578b8"
      },
      "source": [
        "# Select features (columns) to be involved intro training and predictions\n",
        "cols = list(dataset_train)[1:6]\n",
        "\n",
        "# Extract dates (will be used in visualization)\n",
        "datelist_train = list(dataset_train['Date'])\n",
        "datelist_train = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist_train]\n",
        "\n",
        "print('Training set shape == {}'.format(dataset_train.shape))\n",
        "print('All timestamps == {}'.format(len(datelist_train)))\n",
        "print('Featured selected: {}'.format(cols))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape == (4006, 7)\n",
            "All timestamps == 4006\n",
            "Featured selected: ['Open', 'High', 'Low', 'Close', 'Adj Close']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wjq4whF5A-i"
      },
      "source": [
        "Step #2. Data pre-processing\n",
        "Removing all commas and convert data to matrix shape format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3eZwG5O404p"
      },
      "source": [
        "dataset_train = dataset_train[cols].astype(str)\n",
        "for i in cols:\n",
        "    for j in range(0, len(dataset_train)):\n",
        "        dataset_train[i][j] = dataset_train[i][j].replace(',', '')\n",
        "\n",
        "dataset_train = dataset_train.astype(float)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrFFt0dN67jB",
        "outputId": "f552ba6f-ff81-4bc7-cda6-0ef140e95302"
      },
      "source": [
        "# Using multiple features (predictors)\n",
        "training_set = dataset_train.to_numpy()\n",
        "\n",
        "print('Shape of training set == {}.'.format(training_set.shape))\n",
        "training_set"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training set == (4006, 5).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  49.813286,   51.835709,   47.800831,   49.982655,   49.982655],\n",
              "       [  50.316402,   54.336334,   50.062355,   53.95277 ,   53.95277 ],\n",
              "       [  55.168217,   56.528118,   54.321388,   54.495735,   54.495735],\n",
              "       ...,\n",
              "       [1523.130005, 1535.329956, 1498.      , 1513.640015, 1513.640015],\n",
              "       [1500.      , 1518.689941, 1486.310059, 1518.      , 1518.      ],\n",
              "       [1521.619995, 1523.439941, 1498.420044, 1515.550049, 1515.550049]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxybh-ZS4063",
        "outputId": "1edca3da-240f-4f4a-f313-590c4af85b9c"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "\n",
        "sc_predict = StandardScaler()\n",
        "sc_predict.fit_transform(training_set[:, 0:1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.27195197],\n",
              "       [-1.27058974],\n",
              "       [-1.25745309],\n",
              "       ...,\n",
              "       [ 2.71716347],\n",
              "       [ 2.65453724],\n",
              "       [ 2.713075  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsq0Yw49409F",
        "outputId": "a7eec6e2-416c-4824-c464-f8f75eca8e03"
      },
      "source": [
        "# Creating a data structure with 90 timestamps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "n_future = 60   # Number of days we want top predict into the future\n",
        "n_past = 90     # Number of past days we want to use to predict the future\n",
        "\n",
        "for i in range(n_past, len(training_set_scaled) - n_future +1):\n",
        "    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1] - 1])\n",
        "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "print('X_train shape == {}.'.format(X_train.shape))\n",
        "print('y_train shape == {}.'.format(y_train.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape == (3857, 90, 4).\n",
            "y_train shape == (3857, 1).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqtnh94t5OFp"
      },
      "source": [
        "PART 2. Create a model. Training\n",
        "Step #3. Building the LSTM based Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaUH3gCi40-7"
      },
      "source": [
        "# Import Libraries and packages from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "#from keras.optimizers import Adam"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_0ESZV25PYB"
      },
      "source": [
        "# Initializing the Neural Network based on LSTM\n",
        "model = Sequential()\n",
        "\n",
        "# Adding 1st LSTM layer\n",
        "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset_train.shape[1]-1)))\n",
        "\n",
        "# Adding 2nd LSTM layer\n",
        "model.add(LSTM(units=10, return_sequences=False))\n",
        "\n",
        "# Adding Dropout\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# Compiling the Neural Network\n",
        "model.compile(optimizer ='adam', loss='mean_squared_error')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5idsP5J5Y5J"
      },
      "source": [
        "Step #4. Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzacA8_r5PbQ",
        "outputId": "39c47777-3c64-4829-8627-efb5c5e00b74"
      },
      "source": [
        "%%time\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "tb = TensorBoard('logs')\n",
        "\n",
        "history = model.fit(X_train, y_train, shuffle=True, epochs=30, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=256)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "13/13 [==============================] - 8s 341ms/step - loss: 0.1078 - val_loss: 0.9837\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.98367, saving model to weights.h5\n",
            "Epoch 2/30\n",
            "13/13 [==============================] - 3s 233ms/step - loss: 0.0464 - val_loss: 0.6110\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.98367 to 0.61102, saving model to weights.h5\n",
            "Epoch 3/30\n",
            "13/13 [==============================] - 3s 229ms/step - loss: 0.0394 - val_loss: 0.6146\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.61102\n",
            "Epoch 4/30\n",
            "13/13 [==============================] - 3s 231ms/step - loss: 0.0355 - val_loss: 0.7288\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.61102\n",
            "Epoch 5/30\n",
            "13/13 [==============================] - 3s 231ms/step - loss: 0.0327 - val_loss: 0.6046\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.61102 to 0.60464, saving model to weights.h5\n",
            "Epoch 6/30\n",
            "13/13 [==============================] - 3s 236ms/step - loss: 0.0327 - val_loss: 0.5543\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.60464 to 0.55427, saving model to weights.h5\n",
            "Epoch 7/30\n",
            "13/13 [==============================] - 3s 232ms/step - loss: 0.0322 - val_loss: 0.5434\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.55427 to 0.54342, saving model to weights.h5\n",
            "Epoch 8/30\n",
            "13/13 [==============================] - 3s 230ms/step - loss: 0.0330 - val_loss: 0.4583\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.54342 to 0.45833, saving model to weights.h5\n",
            "Epoch 9/30\n",
            "13/13 [==============================] - 3s 229ms/step - loss: 0.0328 - val_loss: 0.4365\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.45833 to 0.43648, saving model to weights.h5\n",
            "Epoch 10/30\n",
            "13/13 [==============================] - 3s 228ms/step - loss: 0.0317 - val_loss: 0.4373\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.43648\n",
            "Epoch 11/30\n",
            "13/13 [==============================] - 3s 230ms/step - loss: 0.0313 - val_loss: 0.3673\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.43648 to 0.36727, saving model to weights.h5\n",
            "Epoch 12/30\n",
            "13/13 [==============================] - 3s 226ms/step - loss: 0.0296 - val_loss: 0.3762\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.36727\n",
            "Epoch 13/30\n",
            "13/13 [==============================] - 3s 225ms/step - loss: 0.0298 - val_loss: 0.3485\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.36727 to 0.34846, saving model to weights.h5\n",
            "Epoch 14/30\n",
            "13/13 [==============================] - 3s 228ms/step - loss: 0.0300 - val_loss: 0.3101\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.34846 to 0.31009, saving model to weights.h5\n",
            "Epoch 15/30\n",
            "13/13 [==============================] - 3s 228ms/step - loss: 0.0297 - val_loss: 0.2976\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.31009 to 0.29757, saving model to weights.h5\n",
            "Epoch 16/30\n",
            "13/13 [==============================] - 3s 228ms/step - loss: 0.0303 - val_loss: 0.3841\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.29757\n",
            "Epoch 17/30\n",
            "13/13 [==============================] - 3s 227ms/step - loss: 0.0310 - val_loss: 0.2552\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.29757 to 0.25522, saving model to weights.h5\n",
            "Epoch 18/30\n",
            "13/13 [==============================] - 3s 225ms/step - loss: 0.0290 - val_loss: 0.3041\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.25522\n",
            "Epoch 19/30\n",
            "13/13 [==============================] - 3s 234ms/step - loss: 0.0297 - val_loss: 0.2497\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.25522 to 0.24969, saving model to weights.h5\n",
            "Epoch 20/30\n",
            "13/13 [==============================] - 3s 228ms/step - loss: 0.0290 - val_loss: 0.2733\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.24969\n",
            "Epoch 21/30\n",
            "13/13 [==============================] - 3s 225ms/step - loss: 0.0294 - val_loss: 0.2279\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.24969 to 0.22793, saving model to weights.h5\n",
            "Epoch 22/30\n",
            "13/13 [==============================] - 3s 229ms/step - loss: 0.0290 - val_loss: 0.2109\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.22793 to 0.21094, saving model to weights.h5\n",
            "Epoch 23/30\n",
            "13/13 [==============================] - 3s 230ms/step - loss: 0.0292 - val_loss: 0.2008\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.21094 to 0.20085, saving model to weights.h5\n",
            "Epoch 24/30\n",
            "13/13 [==============================] - 3s 233ms/step - loss: 0.0277 - val_loss: 0.2021\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.20085\n",
            "Epoch 25/30\n",
            "13/13 [==============================] - 3s 232ms/step - loss: 0.0275 - val_loss: 0.2152\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.20085\n",
            "Epoch 26/30\n",
            "13/13 [==============================] - 3s 230ms/step - loss: 0.0267 - val_loss: 0.2051\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.20085\n",
            "Epoch 27/30\n",
            "13/13 [==============================] - 3s 232ms/step - loss: 0.0280 - val_loss: 0.2329\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.20085\n",
            "Epoch 28/30\n",
            "13/13 [==============================] - 3s 228ms/step - loss: 0.0291 - val_loss: 0.1901\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.20085 to 0.19008, saving model to weights.h5\n",
            "Epoch 29/30\n",
            "13/13 [==============================] - 3s 234ms/step - loss: 0.0272 - val_loss: 0.1865\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.19008 to 0.18647, saving model to weights.h5\n",
            "Epoch 30/30\n",
            "13/13 [==============================] - 3s 229ms/step - loss: 0.0275 - val_loss: 0.1873\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.18647\n",
            "CPU times: user 2min 31s, sys: 10.3 s, total: 2min 41s\n",
            "Wall time: 2min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9Q6Jdy5ggq"
      },
      "source": [
        "Notes:\n",
        "EarlyStopping - Stop training when a monitored metric has stopped improving.\n",
        "monitor - quantity to be monitored.\n",
        "min_delta - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
        "patience - number of epochs with no improvement after which training will be stopped.\n",
        "ReduceLROnPlateau - Reduce learning rate when a metric has stopped improving.\n",
        "factor - factor by which the learning rate will be reduced. new_lr = lr * factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BDzJZo55iGj"
      },
      "source": [
        "The last date for our training set is 30-Dec-2016.\n",
        "We will perform predictions for the next 20 days, since 2017-01-01 to 2017-01-20.\n",
        "\n",
        "PART 3. Make future predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDB8S_2m5W6J"
      },
      "source": [
        "# Generate list of sequence of days for predictions\n",
        "datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='1d').tolist()\n",
        "\n",
        "'''\n",
        "Remeber, we have datelist_train from begining.\n",
        "'''\n",
        "\n",
        "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
        "datelist_future_ = []\n",
        "for this_timestamp in datelist_future:\n",
        "    datelist_future_.append(this_timestamp.date())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4DTEPFb5q6Z"
      },
      "source": [
        "Step #5. Make predictions for future dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8OWrUNf5W9B"
      },
      "source": [
        "# Perform predictions\n",
        "predictions_future = model.predict(X_train[-n_future:])\n",
        "predictions_train = model.predict(X_train[n_past:])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "DJbAX02l5W__",
        "outputId": "7eb0e16c-02e1-4cbf-d8b5-1d1a50372fd1"
      },
      "source": [
        "# Inverse the predictions to original measurements\n",
        "\n",
        "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
        "def datetime_to_timestamp(x):\n",
        "    '''\n",
        "        x : a given datetime value (datetime.date)\n",
        "    '''\n",
        "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
        "\n",
        "\n",
        "y_pred_future = sc_predict.inverse_transform(predictions_future)\n",
        "y_pred_train = sc_predict.inverse_transform(predictions_train)\n",
        "\n",
        "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=['Open']).set_index(pd.Series(datelist_future))\n",
        "PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=['Open']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\n",
        "\n",
        "# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\n",
        "PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)\n",
        "\n",
        "PREDICTION_TRAIN.head(3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2005-08-01</th>\n",
              "      <td>160.492630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-08-02</th>\n",
              "      <td>161.024948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-08-03</th>\n",
              "      <td>161.474136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open\n",
              "2005-08-01  160.492630\n",
              "2005-08-02  161.024948\n",
              "2005-08-03  161.474136"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4UwJ7qa5vo5"
      },
      "source": [
        "Step #6. Visualize the Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI_X1w5J5XB5"
      },
      "source": [
        "# Set plot size \n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 14, 5\n",
        "\n",
        "# Plot parameters\n",
        "START_DATE_FOR_PLOTTING = '2012-06-01'\n",
        "\n",
        "plt.plot(PREDICTIONS_FUTURE.index, PREDICTIONS_FUTURE['Open'], color='r', label='Predicted Stock Price')\n",
        "plt.plot(PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:].index, PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:]['Open'], color='orange', label='Training predictions')\n",
        "plt.plot(dataset_train.loc[START_DATE_FOR_PLOTTING:].index, dataset_train.loc[START_DATE_FOR_PLOTTING:]['Open'], color='b', label='Actual Stock Price')\n",
        "\n",
        "plt.axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')\n",
        "\n",
        "plt.grid(which='major', color='#cccccc', alpha=0.5)\n",
        "\n",
        "plt.legend(shadow=True)\n",
        "plt.title('Predcitions and Acutal Stock Prices', family='Arial', fontsize=12)\n",
        "plt.xlabel('Timeline', family='Arial', fontsize=10)\n",
        "plt.ylabel('Stock Price Value', family='Arial', fontsize=10)\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DGeFEt5XGO"
      },
      "source": [
        "# Parse training set timestamp for better visualization\n",
        "dataset_train = pd.DataFrame(dataset_train, columns=cols)\n",
        "dataset_train.index = datelist_train\n",
        "dataset_train.index = pd.to_datetime(dataset_train.index)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}